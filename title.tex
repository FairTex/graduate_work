\documentclass[14pt]{extreport}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{amssymb,amsfonts,amsmath,mathtext,cite,enumerate,float}
\usepackage{array}
\usepackage{setspace}
\usepackage[dvips]{graphicx}

\makeatletter
\renewcommand{\@biblabel}[1]{#1.} % Заменяем библиографию с квадратных скобок на точку:
\renewcommand{\baselinestretch}{1.5}
\makeatother

\usepackage[left=3.0cm,right=1.5cm,top=2.0cm,bottom=2.0cm,bindingoffset=0cm]{geometry}

%\renewcommand{\theenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
%\renewcommand{\labelenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
%\renewcommand{\theenumii}{.\arabic{enumii}}% Меняем везде перечисления на цифра.цифра
%\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}.}% Меняем везде перечисления на цифра.цифра
%\renewcommand{\theenumiii}{.\arabic{enumiii}}% Меняем везде перечисления на цифра.цифра
%\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.}% Меняем везде перечисления на цифра.цифра

\begin{document}

   \begin{titlepage}{
        \thispagestyle{empty}\newgeometry{left=2.5cm,right=1.5cm,top=0cm,bottom=0.2cm,bindingoffset=0cm}\setstretch{1}
        \begin{center}
            {\footnotesize
                Министерство образования и науки Российской Федерации\\
                Федеральное государственное автономное образовательное учреждение\\
                высшего профессионального образования\\
            }

            <<Уральский федеральный университет\\
             имени первого Президента России Б.Н.Ельцина>>

             \vskip+0.5cm

            Институт математики и компьютерный наук\\
            Кафедра алгебры и дискретной математики

            \vskip+25mm

            {\bf \LARGE
                Применение нейронных сетей для калибровки оборудования на примере двух задач робототехники. \\
            }

            \vskip+15mm
        \end{center}

        \vfill
        \noindent\begin{parbox}[t]{9cm}{\small
                \vspace{2.0cm}
                Допустить к защите:

                \bigskip
                \bigskip
                \bigskip

                \hbox to45mm{\hrulefill}

                \bigskip

                <<\,\hbox to10mm{\hrulefill}\,>>  \hbox to25mm{\hrulefill}  2015 г.
            }
            \end{parbox}
            \begin{parbox}[t]{9cm}{\small  \setstretch{1}
                Выпускная квалификационная \\
                работа на степень бакалавра\\
                 по направлению\\
                %НЕПРАВИЛЬНАЯ ХРЕНОТА -> 01.03.01 \\
                Математика и компьютерные науки\\
                студента группы МК-410502 \\
                                \bigskip
                Штех Геннадия Петровича\\
                Научный руководитель\\
                кандидат физико-математических наук \\
                Окуловский Юрий Сергеевич\\
            }
            \end{parbox}
        \vfill
        \centerline{Екатеринбург}
        \centerline{2015}
        }\restoregeometry
    \end{titlepage}
\newpage
    \tableofcontents

\newpage
    \chapter{О чем диплом?}
        Не редко при конструировнии подвижных платформ применяются двигатели, управляемые постоянным током. Характерной особенностью этих двигателей является их дешевизна и долговечность. Скорость вращения вала двигателя постоянного тока обычно регистрируется с помощью дополнительных устройств. В рассмотренном случае это были энкодеры. Не будем вдаваться в принцип их работы, стоит отметить лишь тот факт, что для текущей задачи их точности хватает с запасом. Главная сложность при их использовании -- это сложность управления непосредственно скоростью вращения вала. Потому что фактически управлять можно только напряжением на контактах двигателя, что хорошо связано только с моментом силы на валу. Однако при использовании двигателей для перемещения необходимо хорошо контролировать именно скорость. Поэтому остро стоит проблема определения подходящего сигнала для достижения необходимой скорости вращения.

        Итак задачу можно формализовать следующим образом: нужно построить функцию, по желаемой скорости и по, возможно, необходимым дополнительным параметрам, вычисляющую напряжение на контактах двигателя, приводящее скорость к желаемой. Для простоты будем называть напряжение на контактах двигателей сигналом.

        Поскольку есть возможность собрать экспериментальные данные об искомой зависимости, подобную задачу восстановления неизвестной функции возможно решить методами регрессии. К качестве используемого метода выберем нейронную сеть. А именно -- простую многослойную нейронную сеть, обучающуюся с учителем, биполярная сигмоида в качестве функции активации.
\newpage
    \chapter{Подробные сведения об используемой нейронной сети}

    Рассматривать обучение нейронной сети будем как задачу оптимизации функции нескольких переменных. Для этого введем некоторые определения:

    \textbf{Постановка задачи}

{\bf Дано:}

\begin{tabular}{p{6cm} p{7.5cm}}
 $\mathcal{X}=(X_1,\ldots,X_k)$ & входные вектора, $X_i\in\mathbb{R}^n$\\[0.1cm]
 $\mathcal{A}=(A_1,\ldots,A_k)$ & правильные выходные вектора, $A_i\in\mathbb{R}^m$\\[0.1cm]
 $(\mathcal{X},\mathcal{A})$ & обучающая выборка  \\[0.1cm]
 $W$ & вектор весов нейронной сети \\[0.1cm]
 $N(W,X)$ & функция, соответствующая нейронной сети \\[0.1cm]
 $Y=N(W,X)$ & ответ нейронной сети, $Y\in\mathbb{R}^m$ \\[0.1cm]
 $D(Y,A) =\sum_{j=1}^{m} (Y[j]-A[j])^2$ & функция ошибки\\[0.1cm]
 $D_i(Y)=D(Y,A_i) $ & функция ошибки на $i$-ом примере \\[0.1cm]
  $E_i(W)=D_i(N(W,X_i))$ & ошибка сети на $i$-ом примере\\[0.1cm]
  $E(W)=\sum_{i=1}^{k}E_i(W) $ & ошибка сети на всей обучающей выборке \\[0.1cm]
 \end{tabular}

{\bf Найти:}

вектор $W$ такой, что $E(W) \rightarrow \min$ (обучение на всей выборке)

вектор $W$ такой, что $E_i(W) \rightarrow \min$ (обучение на одном примере)

Обучение нейронной сети свелось к минимизации вектор-функции $E_i(W)$. Мы должны минимизировать значение функции, подобрав аргумент так, чтобы значение стало минимальным.

Для начала рассмотрим минимизацию функции одной переменной алгоритмом градиентного спуска, затем перейдем к минимизации функции многих переменных.

Начнем с того, что случайным образом выберем точку. Изучим значение производной в этой точке. Напомним, что производная функции в точке --- это число, которое показывает нам, возрастает функция или убывает, то есть если функция возрастает, то производная положительная, если убывает --- отрицательная.
Из выбранной точки сделаем шаг, равный значению производной, в направлении, обратном направлению значения производной в данной точке, получим следующую точку, для которой проделаем все в точности то же, что и для выбранной изначальной точки. На каждом таком шаге значение функции будет убывать, а значит, в конце она достигнет минимума. Таким образом получаем \textbf{алгоритм градиентного спуска:}
\begin{enumerate}
 \item Инициализировать $x_1$ случайным значением из $\mathbb{R}$
 \item $i:=1$, $i$ --- номер итерации
 \item $x_{i+1}=x_i-\varepsilon f'(x_i)$, $-\varepsilon f'(x_i)$ --- направление обратной производной, $-\varepsilon$ обеспечивает малый шаг
 \item $i++$
 \item ${\rm if}\ \ f(x_{i})-f(x_{i+1}) >c\ \ {\rm goto}\ 3$
\end{enumerate}

В случае нескольких переменных шаги будем делать в направлении, обратном направлению градиента, который является неким аналогом производной для многомерного случая.
Пусть $f(x_1, \ldots, x_n)$ --- функция $n-$переменных, $f(x_1, \ldots, x_n)$:$\mathbb{R}^n\rightarrow \mathbb{R}$.
Частная производная по $i-$ой переменной: $$\frac{\partial f}{\partial x_i}(x_1, \ldots, x_n) = $$
$$\lim_{\varepsilon \to  0}[f(x_1, \ldots, x_i + \varepsilon, \ldots, x_n) -- f(x_1, x_2, \ldots, x_i, \ldots, x_n)]/ \varepsilon$$
Частная производная по $i-$ой переменной --- это тоже функция $\frac{\partial f}{\partial x_i}: \mathbb{R}^n\rightarrow \mathbb{R}$.

Градиент функции: $$\nabla f = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right)$$
$$\nabla f:\mathbb{R}^n\rightarrow \mathbb{R}^n$$
Производная сложной функции: $$f(x_1, \ldots, x_n)$$
$$x_i = x_i(y_1, \ldots, y_m)$$
$$f(y_1, \ldots, y_m) = f(x_1(y_1, \ldots, y_m), \ldots, x_n(y_1, \ldots, y_m))$$
$$\frac{\partial f}{\partial y_i} = \sum_{j=1}^{n} \frac{\partial f}{\partial x_j}\frac{\partial x_j}{\partial y_j}$$
\textbf{Алгоритм градиентного спуска} для многомерного случая:
\begin{enumerate}
 \item Инициализировать $W_1$ случайным значением из $\mathbb{R}^n$
 \item $i:=1$
 \item $W_{i+1}=W_i-\varepsilon \nabla f(W_i)$
 \item $i++$
 \item ${\rm if}\ \ f(W_{i})-f(W_{i+1})>c\ \ {\rm goto}\ 3$
 \end{enumerate}

Посчитаем градиент функции ошибки, с помощью которого мы минимизируем функцию ошибки и тем самым подберем те веса, на которых она минимальна, а минимальна она тогда, когда выход практически совпадает с правильным ответом, то есть подберем веса так, чтобы нейронная сеть научилась давать выход, совпадающий с ответом.

\textbf{Обратное распространение ошибки}

Научимся считать градиент ошибки по весам нейронной сети в случае, если она состоит из нескольких слоев. Этот алгоритм называется алгоритмом обратного распространения ошибки, который обеспечил возможность практического применения нейронных сетей, так как нейронные сети из одного слоя могут решать маленькое количество задач, а из нескольких слоев --- большое. Алгоритм градиентного спуска --- алгоритм минимизации любой произвольной функции с помощью градиента, это численный алгоритм, который использует значение градиента, алгоритм обратного распространения ошибки --- алгоритм вычисления градиента для того конкретного случая, когда функция является нейронной сетью.

Функция ошибки в общем случае:
$$D_k(y_1,...,y_n)=(y_1-a_1)^2+...+(y_n-a_n)^2.$$
Производная функции ошибки по $y_i$:
$$\frac{\partial{D_k}}{\partial{y_i}}=2(y_i-a_i).$$
Взвешанная сумма $i-$ого нейрона:
$$S_i=\sum_{j=0}^{m}x_j w_{ji}.$$
Функция активации соответствующего $S_i$:
$$y_i=f(S_i).$$
Производная функции активации по $x_j$:
$$\frac{\partial{y_i}}{\partial{x_j}}={f'(S_i)w_{ji}}.$$
Производная ошибки по входам нейронной сети:
$$\frac{\partial{D_k}}{\partial {x_i}}=\sum_{i=0}^{n}\frac{\partial{D_k}}{\partial {y_i}}\frac{\partial{y_i}}{\partial {x_j}} = 2\sum_{i=0}^{n}(y_i-a_i)f'(S_i)w_{ji}$$

Функция вычисляется на основании своих переменных, каждая из которых вносит определенный вклад в эту функцию, которая и является частной производной по этой переменной.
\newpage
    \chapter{Описание прямого решения задачи с применением нейронной сети}
            \section{Модель}
                За входной вектор возьмем желаемую скорость($Spd$), за выходной вектор возьмём сигнал($Sig$), приводящий к этой скорости.
                \[Sig \in \mathbb{SIG}; Spd \in \mathbb{SPD}\]
                \[\mathbb{SIG} = \{Sig \in \mathbb{N}\vert -256 < Sig < 256\}; \mathbb{SPD} = \mathbb{R}\]
            \section{Описание сценария для сбора экспериментальных данных}
                Соберём экспериментальные данные по следующему сценарию: будем произвольным образом менять сигнал ($Sig_l$)($l$ -- значит last, прошлый сигнал), ждать установления скорости вращения($Spd_c$)($c$ -- значит current, текущая скорость), записывать пару ($Sig_l$, $Spd_c$) в экспериментальную выборку и вновь менять сигнал.

                Фактически мы получили выборку отображения из пространства сигналов в пространство скоростей.
                \[Sig \in \mathbb{SIG}; Spd \in \mathbb{SPD}\]
                \[F(Sig_l) = Spd_c\]
                Но нам нужна обратная функция, по желаемой скорости ($Spd_n$)($n$ -- next, следующая) возвращающая необходимый текущий сигнал ($Sig_c$).
                \[Sig \in \mathbb{SIG}; Spd \in \mathbb{SPD}\]
                \[F^{-1}(Spd_n) = Sig_c\]
                Чтобы регрессировать обратную функцию, надо лишь перевернуть пару так, как там нужно.
                \[(Sig_l, Spd_c) \rightarrow (Spd_c, Sig_l)\]
                В том числе, конечно, необходимо проверить наличие обратной данной функции: исходная функция должна быть монотонной. Монотонность функции очевидна из графика, на котором отображены экспериментальные данные в виде точек.

                {\bfКАРТИНКА С ДАННЫМИ}
                Особенность в районе нуля обусловлена наличием в системе трения и чтобы "сорвать" систему из состояния покоя требуется достаточно высокий момент вращения, который требует достаточно высокого сигнала на двигателях.
            \section{Результат подхода}

                {\bfКАРТИНКА РЕЗУЛЬТИРУЮЩЕЙ ЛИНИИ}
                На практике такой подход показывает себя плохо: поскольку сценарий обучения не соответствует сценарию использования. При управлении платформой скорость двигателей меняется достаточно часто(до двадцати раз в секунду) в достаточно широком диапазоне. Но функция регрессии спроектирована так, что желаемая скорость достигается через некоторое неопределённое время после выставления сигнала. Экспериментально установлено, что в среднем это несколько секунд. При динамическом управлении платформой это неприемлемый результат. Особенно заметна эта особенность при торможении.

                {\bfКАРТИНКА ЛИНИИ ТОРМОЖЕНИЯ}
                Более того, видно, что особенность вокруг нуля нейронная сеть не выучила. Эту особенность удалось выучить только разбиением пространства на 2: $Sig>0$ и $Sig<=0$ с независимым обучением двух сетей на этих пространствах.
            \section{Попытки модификации подхода к формированию экспериментальной выборки}
                Для приведения в соответствие сценария использования и сценария обучения была сделана модификация сценария сбора данных. Изменение: не ждать установления скорости вала и добавлять в выборку все пары ($Sig_l$, $Spd_c$). Этот путь тоже не приводит к нужному результату, поскольку выборка становится зашумлённой и обилие информации нейронная сеть воспринимает как шум. В результате использование этой функции регрессии для управления двигателем ни в каких сценариях не даёт приемлимого результата.
                {\bfКАРТИНКА ТУЧИ ТОЧЕК ХАОС КРОВЬ КИШКИ!!!11}
            \section{Bывод}
                Прямой подход не даёт пригодного к использованию результата. Главным образом из-за того, что сеть не имеет достаточной информации для выдачи сигнала, подходящего к текущей ситуации. Например ситуации ускорения, торможения и равномерного движения требуют различного подхода к формированию сигнала: завышение, занижения и непосредственное соотвествие. В том числе у сети нет временных рамок достижения скорости по сигналу, что даёт непредсказуемое время приведения системы в необходимое состояние. Таким образом мы получили список недостатков:
                \begin{itemize}
                  \item Отсутствие временных рамок
                  \item Отсутствие контекста применения сигнала
                      \begin{itemize}
                        \item Режим езды: ускорение, торможение, равномерное движение
                        \item Предыдущий выставленный сигнал индуцирует в катушке двигателя ток, который тоже нужно преодолеть
                      \end{itemize}
                  \item Сложная особенность в районе нуля
                \end{itemize}
                Все перечисленные проблемы решаются рассмотрением двигателя в терминах управляемых систем. Для управления двигателем в терминах системы введём понятие регулятора и цикла регуляции.
    \chapter{Инверсный нейрорегулятор}
        \section{Описание принципа устройства регулятора}
            Регулятор -- это сущность, которая способна вычислять управляющее воздействие для поддержания управляемой системы в указанном стостоянии.
            Мы будем использовать термин регулятор в следующем ключе: регулятор -- это функция, принимающая три вектора:
            \begin{itemize}
                  \item вектор, описывающий состояние, которое необходимо поддерживать
                    \[\mathbb{S}_n, \mathbb{S}_n \in \mathbb{STATES}\]
                  \item вектор, описывающий текущее состояние управляемой системы
                    \[\mathbb{S}_c, \mathbb{S}_c \in \mathbb{STATES}\]
                  \item вектор, описывающий прошлые управляющие воздействия, оказанные на систему
                    \[\mathbb{C}_l, \mathbb{C}_l \in \mathbb{CONTROLS}\]
            \end{itemize}
            \[F:\,\mathbb{STATES}\times\mathbb{STATES}\times\mathbb{CONTROLS}\,\rightarrow\,\mathbb{CONTROLS}\]

        \section{Описание принципа устройства цикла регуляции}

            В целом жизнь устроена так: чтобы управлять штуками, нужно постоянно контроллировать, как они отреагировали на прошлое воздействие. Иначе штуки будут делать хуету. Фактически, за штуками нужно следить постоянно, с какой-то периодичностью. Поскольку жизнь вокруг штук тоже может поменяться и прежде годное управляющее воздействие прямо сейчас мчит вас в пропасть. Весь этот процесс называется циклом регуляции. Фактически, устроен он так: мы получаем обратную связь от штуки, вспоминаем, в каком состоянии нам необходимо её поддерживать и на основе обратной связи и наших грязных желаний вырабатываем управление. Возможно, даже учитываем наши предыдущие управляющие воздействия.\\
        \section{Нейронная сеть в качестве регулятора}
            В голову пришла мысль о том, что чтобы управлять динамической системой недостаточно просто построить регрессионную модель её отзывов. Оказалось, что наш двигатель -- это классическая динамическая система, требующая построения регулятора для работы. Тут мы малясь загрустили, ибо ПИД-регулятор если брать обобщенный, то там объебаться можно с коеффициентами, а если по уму его строить, то е нас такого ума нет.\\
            Ее мы взяли нейронную сеть и попробовали решить проблемы как умеем: запихали входной вектор(прошлый сигнал, прошлая скорость, текущий сигнал), взяли выходной(текущая скорость), собрали данных по сценарию(сценарий сцукорегулятора). Обучили сеть, а она сука поехала и стали мы счастливые очень. Фактически нейронная сеть стала инверсным нейрорегулятором, а не просто регрессионной калибровкой.
\newpage
    %\chapter{Список литературы}
        \begin{thebibliography}{99}
        %\addcontentsline{toc}{section}{Литература}
            \bibitem{OmatuKhalidUsuoph} Омату С., Халид М. Юсоф Р. {\it Нейроуправление и его приложения} // Издательтво $"$ИПРЖР$"$, 2000.
            \bibitem{Galushkin} Галушкин А.И. {\it Нейронные сети. Основы теории} // Издательство $"$Горячая Линия - Телеком$"$, 2012.
            \bibitem{Khaykin} Хайкин С. {\it Нейронные сети. Полный курс} // Издательство $"$Вильямс$"$, 2006.
            \bibitem{Uorssermen} Уоссермен Ф. {\it Нейрокомпьютерная техника: Теория и практика} // Издательство $"$Мир$"$, 2006.
        \end{thebibliography}
        
\end{document} 